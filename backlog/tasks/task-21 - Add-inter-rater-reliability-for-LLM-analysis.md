---
id: task-21
title: Add inter-rater reliability for LLM analysis
status: To Do
assignee: []
created_date: '2026-01-15 17:07'
labels:
  - analysis
  - validation
dependencies: []
---

## Description

<!-- SECTION:DESCRIPTION:BEGIN -->
Run analysis multiple times with temperature>0 or use multiple models to assess reliability of LLM ratings. This helps understand confidence in the scores.
<!-- SECTION:DESCRIPTION:END -->

## Acceptance Criteria
<!-- AC:BEGIN -->
- [ ] #1 Run same analysis 3x on subset of participants
- [ ] #2 Calculate agreement metrics (Krippendorff alpha or similar)
- [ ] #3 Document reliability per metric
<!-- AC:END -->
