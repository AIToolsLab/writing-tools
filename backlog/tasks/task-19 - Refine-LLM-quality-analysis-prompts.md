---
id: task-19
title: Refine LLM quality analysis prompts
status: To Do
assignee: []
created_date: '2026-01-15 17:07'
labels:
  - analysis
  - llm
dependencies: []
---

## Description

<!-- SECTION:DESCRIPTION:BEGIN -->
Iterate on the prompts in llm_analysis.py based on actual results. Current prompts are first drafts and may need tuning for:
- More consistent scoring
- Better alignment with human judgment
- Scenario-specific nuances
<!-- SECTION:DESCRIPTION:END -->

## Acceptance Criteria
<!-- AC:BEGIN -->
- [ ] #1 Run analysis on sample of 10+ emails
- [ ] #2 Compare LLM ratings to manual review
- [ ] #3 Adjust prompt wording based on discrepancies
<!-- AC:END -->
